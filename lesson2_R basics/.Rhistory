meandes = apply(design, 2, median)
}
else{
meandes = apply(design, 2, mean)
}
if(is.null(hold) == FALSE){
meandes = hold
}
mindes = apply(design, 2, min)
maxdes = apply(design, 2, max)
for (j in 1:nd){
# base matrix of 'best' values to put the sweep values into
basemat = matrix(meandes, nrow=n, ncol=nd , byrow=TRUE)
vec = seq(from=mindes[j], to=maxdes[j], length.out=n)
basemat[ ,j] = vec
oamat = rbind(oamat, basemat)
}
oamat
}
n = 21 # number of points in to vary
X_oaat = oaat.design(design=X, n = n, hold = X.standard)
colnames(X_oaat) = colnames(X)
pred_amazon_oaat = predict(fit_amazon, newdata = X_oaat, type = 'UK')
# Now we can plot the output in a similar way to the original parameters.
# Remember, in each plot, we just want the block where the parameter in
# question is varied.
pdf(file = 'oat_amazon.pdf', width = 8, height = 6)
par(mfrow = c(2,4), las = 1, mar = c(5,3,2,1), oma = c(0.1,3,0.1,0.1))
for(i in 1:ncol(X_oaat)){
# just the points where the parameter varies
ix <- seq(from = ((i*n) - (n-1)), to =  (i*n), by = 1)
plot(X_oaat[ix, i], pred_amazon_oaat$mean[ix], ylim = c(0, 1), bty = 'n',
ylab = '', type = 'l', lwd = 2,
xlab = colnames(X)[i]
)
lines(X_oaat[ix, i], pred_amazon_oaat$upper95[ix], lty = 'dashed')
lines(X_oaat[ix, i], pred_amazon_oaat$lower95[ix], lty = 'dashed')
}
legend('topright', lty = c('solid', 'dashed'), lwd = c(2,1), legend = c('mean','95% CI'))
mtext('Amazon forest fraction', side = 2, line = 1, col = 'black', outer = TRUE, las = 0)
dev.off()
# How does this compare with the sensitivity of one of the other outputs?
# have a look at the South East Asian forest fraction
# Plot both on the same axes, and use transparent solid polygons to
# represent uncertainty
fit_congo = km(~., design = X, response = full_frac$CONGO_MOD_FRAC)
pred_congo_oaat = predict(fit_congo, newdata = X_oaat, type = 'UK')
pdf(file = 'oat_compare.pdf', width = 8, height = 6)
par(mfrow = c(2,4), las = 1, mar = c(5,3,2,1), oma = c(0.1,3,0.1,0.1) )
for(i in 1:ncol(X_oaat)){
# just the points where the parameter varies
ix <- seq(from = ((i*n) - (n-1)), to =  (i*n), by = 1)
plot(X_oaat[ix, i], pred_amazon_oaat$mean[ix], ylim = c(0, 1), bty = 'n',
ylab = '', type = 'l', lwd = 2,
xlab = colnames(X)[i])
col.transp = adjustcolor('black', alpha = 0.4)
polygon(x = c(X_oaat[ix, i], rev(X_oaat[ix, i])),
y =c(pred_amazon_oaat$lower95[ix], rev(pred_amazon_oaat$upper95[ix])),
col = col.transp, border = col.transp)
col.transp = adjustcolor('darkgreen', alpha = 0.4)
points(X_oaat[ix, i], pred_congo_oaat$mean[ix], ylim = c(0, 1), bty = 'n',
ylab = '', type = 'l', lwd = 2,
xlab = colnames(X)[i],
col = 'darkgreen')
polygon(x = c(X_oaat[ix, i], rev(X_oaat[ix, i])),
y =c(pred_congo_oaat$lower95[ix], rev(pred_congo_oaat$upper95[ix])),
col = col.transp, border = col.transp)
}
legend('top', legend = c('Amazon', 'Congo'), col = c('black','darkgreen'),
lty = 'solid', lwd = 1, pch = NA, bty = 'n',
text.col = 'black',
fill = adjustcolor(c('black', 'darkgreen'), alpha = 0.4), border = NA, cex = 1.2)
mtext('Forest fraction', side = 2, line = 1, col = 'black', outer = TRUE, las = 0)
dev.off()
mtext('Forest fraction', side = 2, line = 1, col = 'black', outer = TRUE, las = 0)
library(DiceKriging) # emulator package
source('~/practice/Untitled Folder/sensitivityAnalysis.r')
# sensitivity_examples.R
# Examples of sensitivity analysis using an emulator
# Doug McNeall
# -----------------------------------------------------------
# 0. load data and packages
# -----------------------------------------------------------
library(DiceKriging) # emulator package
# Load the forest fraction data from the climate model FAMOUS
giturl = 'https://github.com/dougmcneall/famous-git/raw/master/famous_forest_fraction.RData'
load(url(giturl))
# input design matrix X
X = full_frac[, 2:8]
# -----------------------------------------------------------
# 1. Marginal plots
# -----------------------------------------------------------
# You can get a good idea of the influence of parameters by
# plotting the output against each parameter (margin) in turn.
# Here is the Amazon as an example.
pdf(file = 'margins_amazon.pdf', width = 8, height = 6)
par(mfrow = c(2,4), las = 1, mar = c(5,3,2,1), oma = c(0.1,3,0.1,0.1) )
for(i in 1:ncol(X)){
plot(X[, i], full_frac$AMAZ_MOD_FRAC, ylim = c(0,1),
xlab = colnames(X)[i],
bty = 'n')
}
mtext('Amazon forest fraction', side = 2, line = 1, col = 'black', outer = TRUE, las = 0)
dev.off()
# -----------------------------------------------------------
# 2. One-at-a-time SA with an emulator
# -----------------------------------------------------------
# Using the emulator allows you to approximate what would happen if you
# only adjusted one parameter at a time, holding all the others constant
# (often at their mean, or 'best' values).
# First, build an emulator using DiceKriging
# The ~. means we use a linear model as a prior, with the
# Gaussian process modelling the deviations.
fit_amazon = km(~., design = X, response = full_frac$AMAZ_MOD_FRAC)
# We making a function that samples from the input space across
# a single parameter at a time, and then predict the outcome.
oaat.design = function(design, n, med=TRUE, hold=NULL){
# function for creating one-at-a-time design matrix
# INPUTS:
# design .... original design (e.g. a latin hypercube or output from expand.grid)
# n ......... number of design points in each dimension
# med ....... Use median rather than mean?
# hold ...... Use supplied value to hold the non-changing points
#
# OUTPUTS:
# ........... (n x nd) rows, nd columns design matrix, sweeping through parameter space
oamat = NULL
nd = ncol(design)
if(med){
meandes = apply(design, 2, median)
}
else{
meandes = apply(design, 2, mean)
}
if(is.null(hold) == FALSE){
meandes = hold
}
mindes = apply(design, 2, min)
maxdes = apply(design, 2, max)
for (j in 1:nd){
# base matrix of 'best' values to put the sweep values into
basemat = matrix(meandes, nrow=n, ncol=nd , byrow=TRUE)
vec = seq(from=mindes[j], to=maxdes[j], length.out=n)
basemat[ ,j] = vec
oamat = rbind(oamat, basemat)
}
oamat
}
n = 21 # number of points in to vary
X_oaat = oaat.design(design=X, n = n, hold = X.standard)
colnames(X_oaat) = colnames(X)
pred_amazon_oaat = predict(fit_amazon, newdata = X_oaat, type = 'UK')
# Now we can plot the output in a similar way to the original parameters.
# Remember, in each plot, we just want the block where the parameter in
# question is varied.
pdf(file = 'oat_amazon.pdf', width = 8, height = 6)
par(mfrow = c(2,4), las = 1, mar = c(5,3,2,1), oma = c(0.1,3,0.1,0.1))
for(i in 1:ncol(X_oaat)){
# just the points where the parameter varies
ix <- seq(from = ((i*n) - (n-1)), to =  (i*n), by = 1)
plot(X_oaat[ix, i], pred_amazon_oaat$mean[ix], ylim = c(0, 1), bty = 'n',
ylab = '', type = 'l', lwd = 2,
xlab = colnames(X)[i]
)
lines(X_oaat[ix, i], pred_amazon_oaat$upper95[ix], lty = 'dashed')
lines(X_oaat[ix, i], pred_amazon_oaat$lower95[ix], lty = 'dashed')
}
legend('topright', lty = c('solid', 'dashed'), lwd = c(2,1), legend = c('mean','95% CI'))
mtext('Amazon forest fraction', side = 2, line = 1, col = 'black', outer = TRUE, las = 0)
dev.off()
# How does this compare with the sensitivity of one of the other outputs?
# have a look at the South East Asian forest fraction
# Plot both on the same axes, and use transparent solid polygons to
# represent uncertainty
fit_congo = km(~., design = X, response = full_frac$CONGO_MOD_FRAC)
pred_congo_oaat = predict(fit_congo, newdata = X_oaat, type = 'UK')
pdf(file = 'oat_compare.pdf', width = 8, height = 6)
par(mfrow = c(2,4), las = 1, mar = c(5,3,2,1), oma = c(0.1,3,0.1,0.1) )
for(i in 1:ncol(X_oaat)){
# just the points where the parameter varies
ix <- seq(from = ((i*n) - (n-1)), to =  (i*n), by = 1)
plot(X_oaat[ix, i], pred_amazon_oaat$mean[ix], ylim = c(0, 1), bty = 'n',
ylab = '', type = 'l', lwd = 2,
xlab = colnames(X)[i])
col.transp = adjustcolor('black', alpha = 0.4)
polygon(x = c(X_oaat[ix, i], rev(X_oaat[ix, i])),
y =c(pred_amazon_oaat$lower95[ix], rev(pred_amazon_oaat$upper95[ix])),
col = col.transp, border = col.transp)
col.transp = adjustcolor('darkgreen', alpha = 0.4)
points(X_oaat[ix, i], pred_congo_oaat$mean[ix], ylim = c(0, 1), bty = 'n',
ylab = '', type = 'l', lwd = 2,
xlab = colnames(X)[i],
col = 'darkgreen')
polygon(x = c(X_oaat[ix, i], rev(X_oaat[ix, i])),
y =c(pred_congo_oaat$lower95[ix], rev(pred_congo_oaat$upper95[ix])),
col = col.transp, border = col.transp)
}
legend('top', legend = c('Amazon', 'Congo'), col = c('black','darkgreen'),
lty = 'solid', lwd = 1, pch = NA, bty = 'n',
text.col = 'black',
fill = adjustcolor(c('black', 'darkgreen'), alpha = 0.4), border = NA, cex = 1.2)
mtext('Forest fraction', side = 2, line = 1, col = 'black', outer = TRUE, las = 0)
dev.off()
install.packages("pse")
oneRun <- function (r, K, Xo) {
+ X <- Xo
+ for (i in 0:20) {
+ X <- X+r*X*(1-X/K)
+ }
+ return (X)
+ }
> modelRun <- function (my.data) {
+ return(mapply(oneRun, my.data[,1], my.data[,2], my.data[,3]))
+ }
oneRun <- function (r, K, Xo) {
X <- Xo
for (i in 0:20) {
X <- X+r*X*(1-X/K)
}
return (X)
}
> modelRun <- function (my.data) {
return(mapply(oneRun, my.data[,1], my.data[,2], my.data[,3]))
}
oneRun <- function (r, K, Xo) {
X <- Xo
for (i in 0:20) {
X <- X+r*X*(1-X/K)
}
return (X)
}
modelRun <- function (my.data) {
return(mapply(oneRun, my.data[,1], my.data[,2], my.data[,3]))
}
library(pse)
myLHS <- LHS(modelRun, factors, 200, q, q.arg, nboot=50)
factors <- c("r", "K", "X0")
q <- c("qnorm", "qnorm", "qunif")
q.arg <- list( list(mean=1.7, sd=0.15), list(mean=40, sd=1),
+ list(min=1, max=50) )
q.arg <- list( list(mean=1.7, sd=0.15), list(mean=40, sd=1),
list(min=1, max=50) )
oneRun <- function (r, K, Xo) {
X <- Xo
for (i in 0:20) {
X <- X+r*X*(1-X/K)
}
return (X)
}
modelRun <- function (my.data) {
return(mapply(oneRun, my.data[,1], my.data[,2], my.data[,3]))
}
library(pse)
myLHS <- LHS(modelRun, factors, 200, q, q.arg, nboot=50)
plotecdf(myLHS)
corplot(myLHS)
corPlot(myLHS)
plotpc(myLHS)
install.packages("plotpc")
plotpc(myLHS)
plotprcc(myLHS)
p <- pic(myLHS)
print(p$pic)
newLHS <- LHS(modelRun, factors, 300, q, q.arg)
(mySbma <- sbma(myLHS, newLHS))
plotecdf(myLHS)
fast99(model = NULL, factors, n, M = 4, omega = NULL,
q = NULL, q.arg = NULL, ...)
## S3 method for class 'fast99'
tell(x, y = NULL, ...)
## S3 method for class 'fast99'
print(x, ...)
## S3 method for class 'fast99'
plot(x, ylim = c(0, 1), ...)
library(sensitivity)
install.packages("sensitivity")
library(sensitivity)
install.packages("sensitivity")
library(sensitivity)
library(sensitivity)
library("ggplot2", lib.loc="~/R/win-library/3.4")
install.packages("sensitivity")
library(ROCR)
library(ROCR)
mydata <- read.csv(file="C:/Users/amanpreet_singh/Documents/practice/Sensitivity Analysis/actual.csv", header=TRUE, sep=",")
mydata
pred <- prediction(mydata$ip1,mydata$ip2)
typeof(mydata)
mydata
dat1 <- data.frame(val=c(mydata[ip1]),label=c(mydata[ip1]))
dat1 <- data.frame(val=c(mydata['ip1']),label=c(mydata[ip1]))
dat1 <- data.frame(val=c(mydata['ip1']),label=c(mydata[ip1]))
mydata[ip1]
mydata['ip1']
dat1 <- data.frame(val=c(mydata['ip1']),label=c(mydata[ip2]))
dat1 <- data.frame(val=c(mydata['ip1']),label=c(mydata['ip2']))
pred <- prediction(dat1$val,dat1$label)
perf = performance(pred,measure="tpr",x.measure="fpr")
library(sensitivity)
install.packages("sensitivity")
install.packages("sensitivity")
mydata <- read.csv(file="C:/Users/amanpreet_singh/Documents/practice/Sensitivity Analysis/actual.csv", header=TRUE, sep=",")
library(ROCR)
library(ROCR)
mydata <- read.csv(file="C:/Users/amanpreet_singh/Documents/practice/Sensitivity Analysis/actual.csv", header=TRUE, sep=",")
mydata['ip1']
dat1 <- data.frame(val=c(mydata['ip1']),label=c(mydata['ip2']))
pred <- prediction(dat1$val,dat1$label)
mydata[0]
mydata[1]
cf <- mydata[1]
cf
cf[0]
cf[1]
cf[2]
cf <- mydata[2]
cf[2]
cf[1]
install.packages("sensitivity")
library(sensitivity)
as.data.frame.fast99 <- function(x, ...) {
if (!is.null(x$y)) {
S <- data.frame(X=colnames(x$X), x$D1/x$V, 1 - x$Dt/x$V)
colnames(S)[-1] <- c("first.order", "total.order")
S
}
}
dd<-as.data.frame(x)
dd<-as.data.frame.fast99(x)
x <- fast99(model = ishigami.fun, factors = 3, n = 1000,
q = "qunif", q.arg = list(min = -pi, max = pi))
print(x)
as.data.frame.fast99 <- function(x, ...) {
if (!is.null(x$y)) {
S <- data.frame(X=colnames(x$X), x$D1/x$V, 1 - x$Dt/x$V)
colnames(S)[-1] <- c("first.order", "total.order")
S
}
}
dd<-as.data.frame.fast99(x)
library(ggplot2)
ggplot(dd, aes(x=X)) +
geom_point(aes(y=first.order, color="first")) +
geom_point(aes(y=total.order, color="total")) +
scale_color_manual(values=c(first="red",total="blue"), name="order")
x <- fast99(model = ishigami.fun, factors = 4, n = 1000,
q = "qunif", q.arg = list(min = -pi, max = pi))
print(x)
as.data.frame.fast99 <- function(x, ...) {
if (!is.null(x$y)) {
S <- data.frame(X=colnames(x$X), x$D1/x$V, 1 - x$Dt/x$V)
colnames(S)[-1] <- c("first.order", "total.order")
S
}
}
dd<-as.data.frame.fast99(x)
library(ggplot2)
ggplot(dd, aes(x=X)) +
geom_point(aes(y=first.order, color="first")) +
geom_point(aes(y=total.order, color="total")) +
scale_color_manual(values=c(first="red",total="blue"), name="order")
udacious <- c("Chris Saden", "Lauren Castellano",
"Sarah Spikes","Dean Eckles",
"Andy Brown", "Moira Burke",
"Kunal Chawla")
numbers <- c(1:10)
numbers
numbers <- c(numbers, 11:20)
numbers
udacious <- c("Chris Saden", "Lauren Castellano",
"Sarah Spikes","Dean Eckles",
"Andy Brown", "Moira Burke",
"Kunal Chawla", "Amanpreet Singh")
mystery = nchar(udacious)
mystery
mystery == 11
udacious[mystery == 11]
data(mtcars)
names(mtcars)
?mtcars
mtcars
str(mtcars)
dim(mtcars)
?row.names
row.names(mtcars) <- c(1:32)
mtcars
row.names(mtcars) <- c(1:33)
row.names(mtcars) <- c(1:32)
mtcars
data(mtcars)
head(mtcars, 10)
head(mtcars)
tail(mtcars, 3)
mtcars$mpg
mtcars$hp
mtcars$wt
mean(mtcars$mpg)
install.packages(swirl)
install.packages("swirl")
install.packages("swirl")
getwd()
getwd()
setwd('C:\Users\amanpreet_singh\Documents\Data Analysis with R\lesson2')
setwd('C:/Users/amanpreet_singh/Documents/Data Analysis with R/lesson2')
getwd()
source('~/Data Analysis with R/lesson2/demystifying.r')
getwd()
install.packages("swirl")
library(swirl)
ls
ls()
rm(list=ls())
swirl()
source('~/Data Analysis with R/lesson2/demystifying.r')
setwd('C:/Users/amanpreet_singh/Documents/Data Analysis with R/lesson2')
getwd()
rm(list=ls())
getwd()
statesInfo <- read.csv('statesData.csv')
statesInfo <- read.csv('stateData.csv')
View(statesInfo)
View(statesInfo)
View(statesInfo)
subset(statesInfo, state.region==1)
stateSubset <- subset(statesInfo, state.region==1)
View(stateSubset)
stateSubsetBracket <- statesInfo[statesInfo$state.region==1, ]
View(stateSubsetBracket)
stateSubsetBracket <- statesInfo[statesInfo$illiteracy<==1, ]
stateSubsetBracket <- statesInfo[statesInfo$illiteracy<1, ]
View(stateSubsetBracket)
summary(statesInfo)
getwd()
rm(list=ls())
getwd()
```{r}
?cars
str(cars)
plot(cars)
?cars
str(cars)
summary(mtcars)
efficient <- subset(mtcars, mtcars$mpg>=23)
efficient
length(efficient)
subset(mtcars, mpg > 30 & hp > 100)
subset(mtcars, mpg < 14 | disp > 390)
efficient <- subset(mtcars, mpg>=23)
efficient
subset(mtcars, qsec<=16.90)
lightcars <- subset(mtcars, wt<2)
length(lightcars)
lightcars
mtcars$year <- 1974
mtcars <- subset(mtcars, select = -year)
mtcars$year <- c(1973, 1974)
View(mtcars)
mtcars <- subset(mtcars, select = -year)
mtcars$wt
cond <- mtcars$wt < 3
cond
mtcars$weight_class <- ifelse(cond, 'light', 'average')
mtcars$weight_class
cond <- mtcars$wt > 3.5
mtcars$weight_class <- ifelse(cond, 'heavy', mtcars$weight_class)
mtcars$weight_class
rm(cond)
rm(efficient)
subset(mtcars, mpg>=30 | hp<60)
nrow(efficient)
summary(mtcars)
efficient <- subset(mtcars, mpg>=23)
efficient
nrow(efficient)
subset(mtcars, mpg > 30 & hp > 100)
subset(mtcars, mpg < 14 | disp > 390)
subset(mtcars, qsec<=16.90)
lightcars <- subset(mtcars, wt<2)
nrow(lightcars)
lightcars
mtcars$year <- 1974
mtcars <- subset(mtcars, select = -year)
mtcars$year <- c(1973, 1974)
mtcars <- subset(mtcars, select = -year)
mtcars$wt
cond <- mtcars$wt < 3
cond
mtcars$weight_class <- ifelse(cond, 'light', 'average')
mtcars$weight_class
cond <- mtcars$wt > 3.5
mtcars$weight_class <- ifelse(cond, 'heavy', mtcars$weight_class)
mtcars$weight_class
rm(cond)
rm(efficient)
subset(mtcars, mpg>=30 | hp<60)
rm(list=ls())
getwd()
reddit <- read.csv('reddit.csv')
str(reddit)
table(reddit$employment.status)
summary(reddit)
levels(reddit$age.range)
library(ggplot2)
qplot(data=reddit, x=age.range)
is.factor(reddit$age.range)
levels(reddit$age.range)
ord <- ordered(reddit$age.range)
reddit$age.range <- ordered(reddit$age.range)
qplot(data=reddit, x=age.range)
reddit$age.range <- ordered(reddit$age.range, levels("Under 18","18-24","25-34","35-44","45-54","55-64","65 or Above"))
reddit$age.range <- ordered(reddit$age.range, levels=c("Under 18","18-24","25-34","35-44","45-54","55-64","65 or Above"))
qplot(data=reddit, x=age.range)
